{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "import transformers\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test_text.csv')\n",
    "validation_df = pd.read_csv('../data/validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Fine-Tuning mBART MLSUM`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1` Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets spacy\n",
    "# !python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dabereabasse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('french'))  # Use French stopwords\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    text = re.sub(url_pattern, '', text)  # Remove URLs\n",
    "    html_pattern = r'<.*?>'\n",
    "    text = re.sub(html_pattern, '', text)  # Remove HTML tags\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    text = re.sub(punctuation_pattern, '', text)  # Remove punctuation\n",
    "    number_pattern = r'\\d+'\n",
    "    text = re.sub(number_pattern, '', text)  # Remove numbers\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    text = ' '.join([word.lemma_ for word in nlp(text)])  # Lemmatization\n",
    "    return text\n",
    "\n",
    "# preprocess the text\n",
    "## train_df['preprocessed_text'] = train_df['text'].apply(preprocess)\n",
    "## validation_df['preprocessed_text'] = validation_df['text'].apply(preprocess)\n",
    "\n",
    "# load the preprocessed text\n",
    "train_df = pd.read_csv('../recup/train_preprocessed.csv')\n",
    "validation_df = pd.read_csv('../recup/validation_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2` pretrained mBart mlsum Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import SummarizationPipeline\n",
    "\n",
    "model_name = 'lincoln/mbart-mlsum-automatic-summarization'\n",
    "\n",
    "# Load model and tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create summarization pipeline\n",
    "summarizer = SummarizationPipeline(loaded_model, loaded_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained mBart on text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [11:26<00:00, 52.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge-L F-Score with basic mBART: 0.19894247120905928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate titles for the N first texts in the validation set\n",
    "validation_texts_titles = []\n",
    "batch_size = 8\n",
    "N = 100\n",
    "\n",
    "for i in tqdm(range(0, min(N, len(validation_df)), batch_size)):\n",
    "    # max length of the input text is 512 based on FirstAnalysis.ipynb\n",
    "    batch = [text[:512] for text in validation_df['text'][i:i+batch_size]]\n",
    "    titles = summarizer(batch, max_length=39, min_length=24, num_beams=4, length_penalty=2.0, early_stopping=True, no_repeat_ngram_size=3, top_k=50)\n",
    "    validation_texts_titles.extend([title['summary_text'] for title in titles])\n",
    "\n",
    "# calculate ROUGE scores between the generated titles and the true titles\n",
    "text_mbart_rouge = []\n",
    "for idx, title in validation_df['titles'][0:N].items():\n",
    "    text_mbart_rouge.append(scorer.score(validation_texts_titles[idx], title)['rougeL'][2])\n",
    "\n",
    "avg_rouge_score_text_mbart = sum(text_mbart_rouge) / len(text_mbart_rouge)\n",
    "print(\"Average Rouge-L F-Score with basic mBART:\", avg_rouge_score_text_mbart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained mBart on preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [10:42<00:00, 49.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge-L F-Score with preprocessed mBART: 0.12606814101652428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate titles for the N first preprocessed texts in the validation set\n",
    "validation_preprocessed_texts_titles = []\n",
    "batch_size = 8\n",
    "N = 100\n",
    "\n",
    "for i in tqdm(range(0, min(N, len(validation_df)), batch_size)):\n",
    "    # max length of the input text is 512 based on FirstAnalysis.ipynb\n",
    "    batch = [text[:512] for text in validation_df['preprocessed_text'][i:i+batch_size]]\n",
    "    titles = summarizer(batch, max_length=39, min_length=24, num_beams=4, length_penalty=2.0, early_stopping=True, no_repeat_ngram_size=3, top_k=50)\n",
    "    validation_preprocessed_texts_titles.extend([title['summary_text'] for title in titles])\n",
    "\n",
    "# calculate ROUGE scores between the generated titles and the true titles\n",
    "preprocessed_mbart_rouge = []\n",
    "for idx, title in validation_df['titles'][0:N].items():\n",
    "    preprocessed_mbart_rouge.append(scorer.score(validation_preprocessed_texts_titles[idx], title)['rougeL'][2])\n",
    "\n",
    "avg_rouge_score_preprocessed_mbart = sum(preprocessed_mbart_rouge) / len(preprocessed_mbart_rouge)\n",
    "print(\"Average Rouge-L F-Score with preprocessed mBART:\", avg_rouge_score_preprocessed_mbart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
