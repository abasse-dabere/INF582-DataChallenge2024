{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<h1><br/></h1>\n",
        "<h1>INF 582: Introduction To Text Mining And NLP</h1>\n",
        "<h2>Challenge: News Articles Title Generation</h2>\n",
        "<h3>Notebook: Fine-Tuning the 1st model: mbart-mlsum-automatic-summarization</h3>\n",
        "<h4>Students Name: <br>\n",
        "<b>DABERE Abasse<br>\n",
        "FUERTES PANIZO Cynthia Yacel</b> </h4>\n",
        "<br>\n",
        "</center>"
      ],
      "metadata": {
        "id": "-TYKjdgbTZSz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQnV9jW3JXWc"
      },
      "source": [
        "# `Fine-Tuning mbart-mlsum-automatic-summarization`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K9dUb9-JXWh"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'lincoln/mbart-mlsum-automatic-summarization'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Frjtv_HOJsNg"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U bitsandbytes\n",
        "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "%pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuOlwtYaJXWi",
        "outputId": "306dff5d-831c-4bbe-b086-b30fe6cf0bff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/eleves-a/2021/abasse.dabere/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-03-30 19:47:40.322739: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-30 19:47:40.322777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-30 19:47:40.323464: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-30 19:47:40.327717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-30 19:47:41.461244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Libraries for data analysis and visualization\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Deep learning and natural language processing\n",
        "import torch\n",
        "torch.cuda.empty_cache()  # Clears CUDA cache\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, SummarizationPipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import datasets\n",
        "\n",
        "# Utilities and text preprocessing\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Model evaluation and custom models\n",
        "from rouge_score import rouge_scorer\n",
        "from peft import LoraConfig, TaskType, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PMnbgWrSgCb",
        "outputId": "5546390f-d169-4c18-d256-7616f89ffc3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SYp9DkhSgCc"
      },
      "outputs": [],
      "source": [
        "# rouge score\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'])\n",
        "\n",
        "def calculate_rouge_score(reference, generated):\n",
        "    rouge_score = scorer.score(generated, reference)['rougeL'][2]\n",
        "    return rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMEY0aUOSgCc"
      },
      "outputs": [],
      "source": [
        "# tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.eos_token = tokenizer.pad_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kpl__7NSgCc"
      },
      "source": [
        "## `1 DATA PREPARATION`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxsirRu3SgCc"
      },
      "outputs": [],
      "source": [
        "path = \"data/\"\n",
        "train_df = pd.read_csv(path + 'train.csv')\n",
        "validation_df = pd.read_csv(path + 'validation.csv')\n",
        "test_df = pd.read_csv(path + 'test_text.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFaaiiOMJXWj"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"### Summarize: {text} \\n### Summary:\"\n",
        "\n",
        "def create_dataset_dict(df):\n",
        "    dataset = {'prompt': []}\n",
        "    if 'titles' in df.columns:\n",
        "        dataset['target'] = []\n",
        "    for i in range(len(df)):\n",
        "        dataset['prompt'].append(prompt_template.format(text=df.iloc[i]['text']))\n",
        "        if 'titles' in df.columns:\n",
        "            dataset['target'].append(df.iloc[i]['titles'])\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INLY6wNLSgCc"
      },
      "outputs": [],
      "source": [
        "max_text_length = 512\n",
        "max_title_length = 128\n",
        "\n",
        "# tokenize function\n",
        "def tokenize_function(examples):\n",
        "    tokenizer_inputs = tokenizer(\n",
        "        examples['prompt'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_text_length,\n",
        "    )\n",
        "    if 'target' in examples:\n",
        "        tokenizer_inputs['labels'] = tokenizer(\n",
        "            examples['target'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=max_title_length,\n",
        "        )['input_ids']\n",
        "    return tokenizer_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK4E560KSgCd"
      },
      "outputs": [],
      "source": [
        "def create_dataset(df):\n",
        "    dataset_dict = create_dataset_dict(df)\n",
        "    dataset = datasets.Dataset.from_dict(dataset_dict)\n",
        "    dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        batch_size=4,\n",
        "        drop_last_batch=True,\n",
        "        )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upTBXL4PSgCd",
        "outputId": "38fa3c56-9cdf-466f-a076-b1c3146ebc46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 21400/21400 [00:24<00:00, 861.41 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 21400 samples for train_dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1500/1500 [00:01<00:00, 1025.71 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 1500 samples for validation_dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1500/1500 [00:01<00:00, 1228.24 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 1500 samples for test_dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "train_dataset = create_dataset(train_df)\n",
        "print(f'Created {len(train_dataset[\"prompt\"])} samples for train_dataset')\n",
        "\n",
        "# validation\n",
        "validation_dataset = create_dataset(validation_df)\n",
        "print(f'Created {len(validation_dataset[\"prompt\"])} samples for validation_dataset')\n",
        "\n",
        "# test\n",
        "test_dataset = create_dataset(test_df)\n",
        "print(f'Created {len(test_dataset[\"prompt\"])} samples for test_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkzmSBDwSgCe"
      },
      "outputs": [],
      "source": [
        "def generate_titles(model, N=200, batch_size = 8):\n",
        "    pipeline = SummarizationPipeline(model, tokenizer, device=device)\n",
        "    # generate titles for train\n",
        "    generated_train_titles = []\n",
        "    for i in tqdm(range(0, min(N, len(train_dataset['prompt'])), batch_size)):\n",
        "        batch = [text[:max_text_length] for text in train_dataset['prompt'][i:i+batch_size]]\n",
        "        titles = pipeline(batch, max_length=max_title_length, num_beams=4, length_penalty=2.0, early_stopping=True, top_k=50)\n",
        "        generated_train_titles.extend([title['summary_text'] for title in titles])\n",
        "\n",
        "    # generate titles for validation\n",
        "    generated_validation_titles = []\n",
        "    for i in tqdm(range(0, min(N, len(validation_dataset['prompt'])), batch_size)):\n",
        "        batch = [text[:max_text_length] for text in validation_dataset['prompt'][i:i+batch_size]]\n",
        "        titles = pipeline(batch, max_length=max_title_length, num_beams=4, length_penalty=2.0, early_stopping=True, top_k=50)\n",
        "        generated_validation_titles.extend([title['summary_text'] for title in titles])\n",
        "\n",
        "    return generated_train_titles, generated_validation_titles\n",
        "\n",
        "def generate_test_titles(model, batch_size = 8):\n",
        "    pipeline = SummarizationPipeline(model, tokenizer, device=device)\n",
        "    # generate titles for test\n",
        "    generated_test_titles = []\n",
        "    for i in tqdm(range(0, len(test_dataset['prompt']), batch_size)):\n",
        "        batch = [text[:max_text_length] for text in test_dataset['prompt'][i:i+batch_size]]\n",
        "        titles = pipeline(batch, max_length=max_title_length, num_beams=4, length_penalty=2.0, early_stopping=True, top_k=50)\n",
        "        generated_test_titles.extend([title['summary_text'] for title in titles])\n",
        "\n",
        "    return generated_test_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9_CJc6uSgCe"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge(generated_train_titles, generated_validation_titles):\n",
        "    # rouge score for train\n",
        "    train_rouge_scores = []\n",
        "    N = len(generated_train_titles)\n",
        "    for i in tqdm(range(N)):\n",
        "        rouge_score = calculate_rouge_score(train_df['titles'][i], generated_train_titles[i])\n",
        "        train_rouge_scores.append(rouge_score)\n",
        "    avg_train_rouge_score = sum(train_rouge_scores) / len(train_rouge_scores)\n",
        "\n",
        "    # rouge score for validation\n",
        "    validation_rouge_scores = []\n",
        "    N = len(generated_validation_titles)\n",
        "    for i in tqdm(range(N)):\n",
        "        rouge_score = calculate_rouge_score(validation_df['titles'][i], generated_validation_titles[i])\n",
        "        validation_rouge_scores.append(rouge_score)\n",
        "    avg_validation_rouge_score = sum(validation_rouge_scores) / len(validation_rouge_scores)\n",
        "\n",
        "    return avg_train_rouge_score, avg_validation_rouge_score\n",
        "\n",
        "# Store the generated summaries in the Kaggle-accepted format\n",
        "def generate_submission_df(generated_test_titles):\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df['ID'],\n",
        "        'titles': generated_test_titles,\n",
        "    })\n",
        "    return submission_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnYRwWlCSgCe"
      },
      "source": [
        "## `3 Test the Base Model`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puyIfPbSSgCe",
        "outputId": "70a9abf1-7213-467d-d519-c6da85006f25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/eleves-a/2021/abasse.dabere/.local/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "# base model\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcSnEb7aSgCe",
        "outputId": "df66cbc2-9a12-4236-8b44-4d7ebf82bd66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 1/25 [00:03<01:32,  3.85s/it]Your max_length is set to 128, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
            "Your max_length is set to 128, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            "  8%|▊         | 2/25 [00:07<01:21,  3.53s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            " 20%|██        | 5/25 [00:18<01:17,  3.86s/it]Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 24%|██▍       | 6/25 [00:22<01:09,  3.63s/it]Your max_length is set to 128, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
            " 28%|██▊       | 7/25 [00:26<01:07,  3.73s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 32%|███▏      | 8/25 [00:29<01:01,  3.63s/it]Your max_length is set to 128, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            "Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
            " 36%|███▌      | 9/25 [00:32<00:57,  3.58s/it]Your max_length is set to 128, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            "Your max_length is set to 128, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
            " 40%|████      | 10/25 [00:36<00:55,  3.73s/it]/users/eleves-a/2021/abasse.dabere/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            " 44%|████▍     | 11/25 [00:40<00:51,  3.69s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
            " 48%|████▊     | 12/25 [00:44<00:47,  3.69s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 52%|█████▏    | 13/25 [00:48<00:45,  3.76s/it]Your max_length is set to 128, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            " 56%|█████▌    | 14/25 [00:51<00:40,  3.71s/it]Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 60%|██████    | 15/25 [00:55<00:36,  3.66s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 68%|██████▊   | 17/25 [01:02<00:28,  3.55s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 76%|███████▌  | 19/25 [01:09<00:20,  3.47s/it]Your max_length is set to 128, but your input_length is only 125. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            "Your max_length is set to 128, but your input_length is only 106. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n",
            " 88%|████████▊ | 22/25 [01:19<00:10,  3.59s/it]Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 92%|█████████▏| 23/25 [01:23<00:07,  3.63s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 96%|█████████▌| 24/25 [01:27<00:03,  3.64s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "100%|██████████| 25/25 [01:31<00:00,  3.65s/it]\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]Your max_length is set to 128, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            "  4%|▍         | 1/25 [00:03<01:30,  3.77s/it]Your max_length is set to 128, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
            "Your max_length is set to 128, but your input_length is only 120. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            " 12%|█▏        | 3/25 [00:10<01:18,  3.55s/it]Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
            " 16%|█▌        | 4/25 [00:14<01:15,  3.60s/it]Your max_length is set to 128, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            " 20%|██        | 5/25 [00:17<01:09,  3.50s/it]Your max_length is set to 128, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            "Your max_length is set to 128, but your input_length is only 125. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            " 36%|███▌      | 9/25 [00:33<01:00,  3.76s/it]Your max_length is set to 128, but your input_length is only 125. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            " 40%|████      | 10/25 [00:37<00:57,  3.85s/it]Your max_length is set to 128, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
            " 48%|████▊     | 12/25 [00:44<00:47,  3.69s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            " 52%|█████▏    | 13/25 [00:48<00:44,  3.75s/it]Your max_length is set to 128, but your input_length is only 120. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            " 68%|██████▊   | 17/25 [01:03<00:29,  3.65s/it]Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n",
            " 72%|███████▏  | 18/25 [01:06<00:25,  3.63s/it]Your max_length is set to 128, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
            " 80%|████████  | 20/25 [01:13<00:17,  3.59s/it]Your max_length is set to 128, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            " 84%|████████▍ | 21/25 [01:17<00:14,  3.67s/it]Your max_length is set to 128, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
            " 96%|█████████▌| 24/25 [01:29<00:03,  3.93s/it]Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "Your max_length is set to 128, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
            "100%|██████████| 25/25 [01:33<00:00,  3.74s/it]\n"
          ]
        }
      ],
      "source": [
        "# generate titles for train and validation\n",
        "generated_validation_titles, generated_train_titles = generate_titles(\n",
        "    base_model,\n",
        "    N=200,\n",
        "    batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaQKgVmOSgCf",
        "outputId": "dc79725c-4556-42cb-fc78-f69af3212ba5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 2656.56it/s]\n",
            "100%|██████████| 200/200 [00:00<00:00, 2851.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average rouge score for train: 0.10547439239641263\n",
            "Average rouge score for validation: 0.10519317043300289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# calculate rouge score\n",
        "avg_train_rouge_score, avg_validation_rouge_score = calculate_rouge(\n",
        "    generated_train_titles,\n",
        "    generated_validation_titles\n",
        ")\n",
        "\n",
        "print(f'Average rouge score for train: {avg_train_rouge_score}')\n",
        "print(f'Average rouge score for validation: {avg_validation_rouge_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sNL92ihJXWm"
      },
      "source": [
        "## `2 PEFT with LORA`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HfAs8gDSgCf"
      },
      "outputs": [],
      "source": [
        "task_type = TaskType.SEQ_2_SEQ_LM\n",
        "lora_rank = 2\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.01\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.1\n",
        "target_modules = ['k_proj', 'v_proj', 'q_proj']\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 8\n",
        "\n",
        "output_dir = f'mbart-mlsum-automatic-summarization/mt{max_text_length}-ms{max_title_length}-lora{lora_rank}/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB40Jh0UJXWn",
        "outputId": "4e853d2e-d42a-4992-ccc1-caab70de5d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 442,368 || all params: 611,321,856 || trainable%: 0.07236253630689755\n"
          ]
        }
      ],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=task_type,\n",
        "    inference_mode=False,\n",
        "    target_modules= target_modules,\n",
        "    r=lora_rank,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    )\n",
        "\n",
        "peft_model = get_peft_model(base_model, peft_config).to(device)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez3toQRMJXWn"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=weight_decay,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE4T-A4aWhrX"
      },
      "source": [
        "- fine-tune the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xURoD1UJXWn"
      },
      "outputs": [],
      "source": [
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7TOgFqfSgCh"
      },
      "outputs": [],
      "source": [
        "# last_checkpoint = # TODO: Set the last checkpoint manually\n",
        "\n",
        "# with open(f'{output_dir}checkpoint-{last_checkpoint}/trainer_state.json') as f:\n",
        "#     training_state = json.load(f)\n",
        "\n",
        "# log_history = training_state['log_history']\n",
        "# # plot the training loss and validation loss over the epochs\n",
        "# epochs_loss = [(log['epoch'], log['loss']) for log in log_history if 'loss' in log]\n",
        "# epochs_val_loss = [(log['epoch'], log['eval_loss']) for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "# plt.plot(*zip(*epochs_loss), label='Training Loss')\n",
        "# plt.plot(*zip(*epochs_val_loss), label='Validation Loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJBg7WClWhrX"
      },
      "source": [
        "## `3 Test Fine-Tuned Model`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdFuksWGWhrY"
      },
      "outputs": [],
      "source": [
        "# checkpoint\n",
        "# TODO: Choose a checkpoint manually\n",
        "checkpoint = 21410\n",
        "path_checkpoint = f'{output_dir}checkpoint-{checkpoint}'\n",
        "\n",
        "checkpoint_model = AutoModelForSeq2SeqLM.from_pretrained(path_checkpoint).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYqLBbcaSgCi"
      },
      "outputs": [],
      "source": [
        "# generate titles for train and validation\n",
        "generated_validation_titles, generated_train_titles = generate_titles(\n",
        "    checkpoint_model,\n",
        "    N=200,\n",
        "    batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etz1HUhcWhrY"
      },
      "outputs": [],
      "source": [
        "# calculate rouge score\n",
        "avg_train_rouge_score, avg_validation_rouge_score = calculate_rouge(\n",
        "    generated_train_titles,\n",
        "    generated_validation_titles\n",
        ")\n",
        "\n",
        "print(f'Average rouge score for train: {avg_train_rouge_score}')\n",
        "print(f'Average rouge score for validation: {avg_validation_rouge_score}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}